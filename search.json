[
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Content",
    "text": "Content\n\nMy background\nEndoscopic Pituitary Tumor Surgery\nHoloscan-platform\nEnd-to-end-apps for surgery\nDemos\nEvent announcement\n\n\n\nContent"
  },
  {
    "objectID": "index.html#my-background",
    "href": "index.html#my-background",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "My background",
    "text": "My background\n\n\nMy background"
  },
  {
    "objectID": "index.html#endoscopic-pituitary-surgery",
    "href": "index.html#endoscopic-pituitary-surgery",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "‚öïÔ∏è Endoscopic Pituitary Surgery",
    "text": "‚öïÔ∏è Endoscopic Pituitary Surgery\n\n\n94,961 views 20 Nov 2012 Barrow Neurological Institute Neurosurgeon Andrew S. Little, MD, demonstrates the process of removing a tumor of the pituitary gland using minimally-invasive endoscopic neurosurgery. https://www.youtube.com/watch?app=desktop&v=EwlRdxokdGk\n553,519 views 28 May 2017 The pituitary gland is located at the bottom of your brain and above the inside of your nose. Endoscopic pituitary surgery (also called transsphenoidal endoscopic surgery) is a minimally invasive surgery performed through the nose and sphenoid sinus to remove pituitary tumors. https://www.youtube.com/watch?v=lwmgNLwt_ts\nMao, Zhehua, Adrito Das, Mobarakol Islam, Danyal Z. Khan, Simon C. Williams, John G. Hanrahan, Anouk Borg et al.¬†‚ÄúPitSurgRT: real-time localization of critical anatomical structures in endoscopic pituitary surgery.‚Äù International Journal of Computer Assisted Radiology and Surgery (2024): 1-8."
  },
  {
    "objectID": "index.html#real-time-ai-applications-for-surgery",
    "href": "index.html#real-time-ai-applications-for-surgery",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Real-time AI Applications for Surgery",
    "text": "Real-time AI Applications for Surgery\n\n\nFigure¬†1: Development and deployment pipeline for real-time AI apps for surgery\n\nPipeline with development and deployment of real-time AI apps for surgery\n{fig-align=center} {fig-pos=‚Äòb‚Äô} b(bottom) h(here) p(page) t(top)"
  },
  {
    "objectID": "index.html#nvidia-holoscan-platform",
    "href": "index.html#nvidia-holoscan-platform",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "NVIDIA Holoscan platform",
    "text": "NVIDIA Holoscan platform\n\n\nHoloscan-SDK\n\n holoscan-sdk\n holohub\n\nClara-AGX\n\n Clara-AGX DevKit\n Orin-IGX DevKit\n\n\n\nHoloscan platform"
  },
  {
    "objectID": "index.html#bring-your-own-model-byom",
    "href": "index.html#bring-your-own-model-byom",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Bring Your Own Model (BYOM)",
    "text": "Bring Your Own Model (BYOM)\n\nWorkflowPythonYAML\n\n\n\n\n\n\n\n\n\nimport os\nfrom argparse import ArgumentParser\n\nfrom holoscan.core import Application\n\nfrom holoscan.operators import (\n    FormatConverterOp,\n    HolovizOp,\n    InferenceOp,\n    SegmentationPostprocessorOp,\n    VideoStreamReplayerOp,\n)\nfrom holoscan.resources import UnboundedAllocator\n\n\nclass BYOMApp(Application):\n    def __init__(self, data):\n        \"\"\"Initialize the application\n\nParameters\n----------\ndata : Location to the data\n\"\"\"\n\n        super().__init__()\n\n        # set name\n        self.name = \"BYOM App\"\n\n        if data == \"none\":\n            data = os.environ.get(\"HOLOSCAN_INPUT_PATH\", \"../data\")\n\n        self.sample_data_path = data\n\n        self.model_path = os.path.join(os.path.dirname(__file__), \"../model\")\n        self.model_path_map = {\n            \"byom_model\": os.path.join(self.model_path, \"identity_model.onnx\"),\n        }\n\n        self.video_dir = os.path.join(self.sample_data_path, \"racerx\")\n        if not os.path.exists(self.video_dir):\n            raise ValueError(f\"Could not find video data:{self.video_dir=}\")\n\n# Define the workflow\n        self.add_flow(source, viz, {(\"output\", \"receivers\")})\n        self.add_flow(source, preprocessor, {(\"output\", \"source_video\")})\n        self.add_flow(preprocessor, inference, {(\"tensor\", \"receivers\")})\n        self.add_flow(inference, postprocessor, {(\"transmitter\", \"in_tensor\")})\n        self.add_flow(postprocessor, viz, {(\"out_tensor\", \"receivers\")})\n\n\ndef main(config_file, data):\n    app = BYOMApp(data=data)\n    # if the --config command line argument was provided, it will override this config_file\n    app.config(config_file)\n    app.run()\n\n\nif __name__ == \"__main__\":\n    # Parse args\n    parser = ArgumentParser(description=\"BYOM demo application.\")\n    parser.add_argument(\n        \"-d\",\n        \"--data\",\n        default=\"none\",\n        help=(\"Set the data path\"),\n    )\n\n    args = parser.parse_args()\n    config_file = os.path.join(os.path.dirname(__file__), \"byom.yaml\")\n    main(config_file=config_file, data=args.data)\n\n\n%YAML 1.2\nreplayer:  # VideoStreamReplayer\n  basename: \"racerx\"\n  frame_rate: 0 # as specified in timestamps\n  repeat: true # default: false\n  realtime: true # default: true\n  count: 0 # default: 0 (no frame count restriction)\n\npreprocessor:  # FormatConverter\n  out_tensor_name: source_video\n  out_dtype: \"float32\"\n  resize_width: 512\n  resize_height: 512\n\ninference:  # Inference\n  backend: \"trt\"\n  pre_processor_map:\n    \"byom_model\": [\"source_video\"]\n  inference_map:\n    \"byom_model\": [\"output\"]\n\npostprocessor:  # SegmentationPostprocessor\n  in_tensor_name: output\n  # network_output_type: None\n  data_format: nchw\n\nviz:  # Holoviz\n  width: 854\n  height: 480\n  color_lut: [\n    [0.65, 0.81, 0.89, 0.1],\n    ]\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#real-time-ai-for-surgery",
    "href": "index.html#real-time-ai-for-surgery",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": " real-time-ai-for-surgery",
    "text": "real-time-ai-for-surgery\nGetting started docs\n\n\nFigure¬†2: Getting started documentation provide with a range of links to setup, use, run and debug application including github workflow.\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#endoscopic-pituitary-surgery-1",
    "href": "index.html#endoscopic-pituitary-surgery-1",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "üè• Endoscopic pituitary surgery",
    "text": "üè• Endoscopic pituitary surgery\n\nüëÉ Multi-head Modelüåì PhaseNet Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#endoscopic-pituitary-surgery-2",
    "href": "index.html#endoscopic-pituitary-surgery-2",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "üè• Endoscopic pituitary surgery",
    "text": "üè• Endoscopic pituitary surgery\n\nüî± Multi AI models\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "ü§ù Contributing",
    "text": "ü§ù Contributing\n\n\nFigure¬†3: real-time-ai-for-surgery follows the Contributor Covenant Code of Conduct. Contributions, issues and feature requests are welcome.\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#template-for-figures",
    "href": "index.html#template-for-figures",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Template for figures",
    "text": "Template for figures\n\n\nFigure¬†4: This text is part of the caption of this figure. Note that default size of presentation slides is 1200 x 700.\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#template-for-tabsets",
    "href": "index.html#template-for-tabsets",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Template for tabsets",
    "text": "Template for tabsets\n\nTab ATab B\n\n\nContent for Tab A\n\n\nContent for Tab B\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#template-for-tabsets-with-code-blocks",
    "href": "index.html#template-for-tabsets-with-code-blocks",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Template for tabsets with code-blocks",
    "text": "Template for tabsets with code-blocks\n\nCode-block ACode-block B\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#multiple-columns",
    "href": "index.html#multiple-columns",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Multiple columns",
    "text": "Multiple columns\n\n\nLeft column\n\nRight column\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#multiple-columns-with-code-blocks",
    "href": "index.html#multiple-columns-with-code-blocks",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "Multiple columns with code-blocks",
    "text": "Multiple columns with code-blocks\n\n\n#Left column\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n#Right column\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#line-highlighting-10-lines",
    "href": "index.html#line-highlighting-10-lines",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "üöß Line Highlighting (10 lines)",
    "text": "üöß Line Highlighting (10 lines)\n\n\nmatplotlib.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#line-highlighting-n-lines",
    "href": "index.html#line-highlighting-n-lines",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "üöß Line Highlighting (N lines)",
    "text": "üöß Line Highlighting (N lines)\n\n\nunit-test-example.py\n\nimport datetime\nimport unittest\n\nimport pandas as pd\nimport pandas_datareader.data as web\n\ndef get_stock_data(ticker):\n    \"\"\"pull data from stooq\"\"\"\n    df = web.DataReader(ticker, 'yahoo')\n    return df\n\nclass TestGetStockData(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        \"\"\"We only want to pull this data once for each TestCase since it is an expensive operation\"\"\"\n        self.df = get_stock_data('^DJI')\n\n    def test_columns_present(self):\n        \"\"\"ensures that the expected columns are all present\"\"\"\n        self.assertIn(\"Open\", self.df.columns)\n        self.assertIn(\"High\", self.df.columns)\n        self.assertIn(\"Low\", self.df.columns)\n        self.assertIn(\"Close\", self.df.columns)\n        self.assertIn(\"Volume\", self.df.columns)\n\n    def test_non_empty(self):\n        \"\"\"ensures that there is more than one row of data\"\"\"\n        self.assertNotEqual(len(self.df.index), 0)\n\n    def test_high_low(self):\n        \"\"\"ensure high and low are the highest and lowest in the same row\"\"\"\n        ohlc = self.df[[\"Open\",\"High\",\"Low\",\"Close\"]]\n        highest = ohlc.max(axis=1)\n        lowest = ohlc.min(axis=1)\n        self.assertTrue(ohlc.le(highest, axis=0).all(axis=None))\n        self.assertTrue(ohlc.ge(lowest, axis=0).all(axis=None))\n\n    def test_most_recent_within_week(self):\n        \"\"\"most recent data was collected within the last week\"\"\"\n        most_recent_date = pd.to_datetime(self.df.index[-1])\n        self.assertLessEqual((datetime.datetime.today() - most_recent_date).days, 7)\n\nunittest.main()\n\n\nReference for the code!\nhttps://machinelearningmastery.com/a-gentle-introduction-to-unit-testing-in-python/"
  },
  {
    "objectID": "index.html#embedding-yotube-video",
    "href": "index.html#embedding-yotube-video",
    "title": "Real-time AI for surgery with NVIDIA-Holoscan platform",
    "section": "üìπ Embedding Yotube Video",
    "text": "üìπ Embedding Yotube Video\n\n\nAvailable aspect ratios include 1x1, 4x3, 16x9 (the default), and 21x9.\nFurther details to render videos https://quarto.org/docs/authoring/videos.html"
  }
]