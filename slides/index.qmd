---
title: "Real-time AI for surgery with NVIDIA-Holoscan platform"
subtitle: ""

format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    controls: false
    controls-layout: bottom-right
    preview-links: auto
    code-block-height: 650px
    code-line-numbers: true
    code-overflow: scroll
    default-image-extension: svg
    fig-align: center
    logo: favicon.svg
    theme: dark
    #css:
      #- css/default.css
      #- css/callouts-html.css
    #footer: <https://quarto.org>
  gfm:
    author: Miguel Xochicale
---

# {.title-slide .centeredslide background-iframe="https://saforem2.github.io/grid-worms-animation/" loading="lazy"}

::: {style="background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;"}
<span style="color:#939393; font-size:1.5em; font-weight: bold;">Real-time AI for surgery</span>  
<span style="color:#939393; font-size:1.5em; font-weight: bold;">with NVIDIA-Holoscan platform</span>  
<span style="color:#777777; font-size:1.2em; font-weight: normal;">ARC Collaborations, UCL</span>  
[<br>&nbsp;]{style="padding-bottom: 0.5rem;"}  
[{{< fa solid home >}}](http://mxochicale.github.io/) Miguel Xochicale, PhD     
[[{{< fa brands github >}} `mxochicale`](https://github.com/mxochicale/real-time-ai-for-surgery-with-NVIDIA-Holoscan-platform)]{style="border-bottom: 0.5px solid #00ccff;"}
:::

::: footer
[2024-04-20 @ [Link for grid-worms-animation 2023](https://github.com/saforem2/grid-worms-animation/)]{.dim-text style="text-align:left;'}
:::

::: {.notes}
First slide
:::


## Content
1. [My background](#sec-ov)
2. [Endoscopic Pituitary Tumor Surgery](#sec-surg)
3. [Holoscan-platform](#sec-hp)
4. [End-to-end-apps for surgery](#sec-e2e)
5. [Demos](#sec-demos)
6. [Event announcement](#sec-aob)

<!--
6. [Acknowledgement](#sec-aob)
-->


::: {.notes}
Content
:::






## My background

![](figures/mybackground.svg){fig-align=center}

::: {.notes}
My background
:::







## :medical_symbol: Endoscopic Pituitary Surgery
{{< video https://www.youtube.com/embed/EwlRdxokdGk
    start="11"
    width="85%" 
    height="85%"
>}}

::: {.notes}
94,961 views  20 Nov 2012
Barrow Neurological Institute Neurosurgeon Andrew S. Little, MD, demonstrates the process of removing a tumor of the pituitary gland using minimally-invasive endoscopic neurosurgery.
https://www.youtube.com/watch?app=desktop&v=EwlRdxokdGk

553,519 views  28 May 2017
The pituitary gland is located at the bottom of your brain and above the inside of your nose. Endoscopic pituitary surgery (also called transsphenoidal endoscopic surgery) is a minimally invasive surgery performed through the nose and sphenoid sinus to remove pituitary tumors.
https://www.youtube.com/watch?v=lwmgNLwt_ts

Mao, Zhehua, Adrito Das, Mobarakol Islam, Danyal Z. Khan, Simon C. Williams, John G. Hanrahan, Anouk Borg et al. "PitSurgRT: real-time localization of critical anatomical structures in endoscopic pituitary surgery." International Journal of Computer Assisted Radiology and Surgery (2024): 1-8.
:::






## Real-time AI Applications for Surgery
::: {#fig-template}

![](figures/rtai4spipeline.svg)

Development and deployment pipeline for real-time AI apps for surgery
:::

::: {.notes}
Pipeline with development and deployment of real-time AI apps for surgery


{fig-align=center}
{fig-pos='b'}
b(bottom)
h(here)
p(page)
t(top)
:::







## Holoscan platform
:::: {.columns}

::: {.column width="50%"}
Holoscan-SDK

![](figures/holoscan-platform/holohub.svg)

[{{< fa brands github >}} `holoscan-sdk`](
https://github.com/nvidia-holoscan/holoscan-sdk/tree/main
)

[{{< fa brands github >}} `holohub`](
https://github.com/nvidia-holoscan/holohub
)

:::

::: {.column width="50%"}
Clara-AGX

![](figures/holoscan-platform/clara_agx_dev_kit_components.svg)

[{{< fa brands github >}} `Clara-AGX DevKit`](
https://github.com/nvidia-holoscan/holoscan-docs/blob/main/devkits/clara-agx/clara_agx_user_guide.md
)

[{{< fa brands github >}} `Orin-IGX DevKit`](
https://github.com/nvidia-holoscan/holoscan-docs/blob/main/devkits/nvidia-igx-orin/nvidia_igx_orin_user_guide.md
)



:::

::::

::: {.notes}
Holoscan platform
:::





## Bring Your Own Model (BYOM) {.scrollable}
::: {.panel-tabset}



### Workflow

![](figures/byom.svg){fig-align=center}

### Python

```{.python}
import os
from argparse import ArgumentParser

from holoscan.core import Application

from holoscan.operators import (
    FormatConverterOp,
    HolovizOp,
    InferenceOp,
    SegmentationPostprocessorOp,
    VideoStreamReplayerOp,
)
from holoscan.resources import UnboundedAllocator


class BYOMApp(Application):
    def __init__(self, data):
        """Initialize the application

Parameters
----------
data : Location to the data
"""

        super().__init__()

        # set name
        self.name = "BYOM App"

        if data == "none":
            data = os.environ.get("HOLOSCAN_INPUT_PATH", "../data")

        self.sample_data_path = data

        self.model_path = os.path.join(os.path.dirname(__file__), "../model")
        self.model_path_map = {
            "byom_model": os.path.join(self.model_path, "identity_model.onnx"),
        }

        self.video_dir = os.path.join(self.sample_data_path, "racerx")
        if not os.path.exists(self.video_dir):
            raise ValueError(f"Could not find video data:{self.video_dir=}")

# Define the workflow
        self.add_flow(source, viz, {("output", "receivers")})
        self.add_flow(source, preprocessor, {("output", "source_video")})
        self.add_flow(preprocessor, inference, {("tensor", "receivers")})
        self.add_flow(inference, postprocessor, {("transmitter", "in_tensor")})
        self.add_flow(postprocessor, viz, {("out_tensor", "receivers")})


def main(config_file, data):
    app = BYOMApp(data=data)
    # if the --config command line argument was provided, it will override this config_file
    app.config(config_file)
    app.run()


if __name__ == "__main__":
    # Parse args
    parser = ArgumentParser(description="BYOM demo application.")
    parser.add_argument(
        "-d",
        "--data",
        default="none",
        help=("Set the data path"),
    )

    args = parser.parse_args()
    config_file = os.path.join(os.path.dirname(__file__), "byom.yaml")
    main(config_file=config_file, data=args.data)

```

### YAML

```{.python}
%YAML 1.2
replayer:  # VideoStreamReplayer
  basename: "racerx"
  frame_rate: 0 # as specified in timestamps
  repeat: true # default: false
  realtime: true # default: true
  count: 0 # default: 0 (no frame count restriction)

preprocessor:  # FormatConverter
  out_tensor_name: source_video
  out_dtype: "float32"
  resize_width: 512
  resize_height: 512

inference:  # Inference
  backend: "trt"
  pre_processor_map:
    "byom_model": ["source_video"]
  inference_map:
    "byom_model": ["output"]

postprocessor:  # SegmentationPostprocessor
  in_tensor_name: output
  # network_output_type: None
  data_format: nchw

viz:  # Holoviz
  width: 854
  height: 480
  color_lut: [
    [0.65, 0.81, 0.89, 0.1],
    ]
```

:::

::: {.notes}
Speaker notes go here.
:::




## Getting started docs
::: {#fig-template}

![](figures/getting-started.svg){fig-align=center}

Getting started documentation provide with a range of links to setup, use, run and debug application including github workflow. 
:::

::: {.notes}
Speaker notes go here.
:::





## üè• Endoscopic pituitary surgery
::: {.panel-tabset}


### üëÉ Multi-head Model

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}

### üåì PhaseNet Model

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}

:::

::: {.notes}
Speaker notes go here.
:::





## üè• Endoscopic pituitary surgery
::: {.panel-tabset}

### üî± Multi AI models

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}

:::

::: {.notes}
Speaker notes go here.
:::



## ü§ù Contributing
::: {#fig-template}

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}

real-time-ai-for-surgery follows the Contributor Covenant Code of Conduct. Contributions, issues and feature requests are welcome. 
:::

::: {.notes}
Speaker notes go here.
:::





## Template for figures 
::: {#fig-template}

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}

This text is part of the caption of this figure. Note that default size of presentation slides is 1200 x 700.
:::

::: {.notes}
Speaker notes go here.
:::








## Template for tabsets {.smaller}
::: {.panel-tabset}

### Tab A

Content for `Tab A`

### Tab B

Content for `Tab B`

:::

::: {.notes}
Speaker notes go here.
:::




## Template for tabsets with code-blocks {.smaller}
::: {.panel-tabset}

### Code-block A

```{.python}
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

### Code-block B

```{.python}
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

:::

::: {.notes}
Speaker notes go here.
:::







## Multiple columns
:::: {.columns}

::: {.column width="50%"}
Left column


:::

::: {.column width="50%"}
Right column


:::

::::

::: {.notes}
Speaker notes go here.
:::






## Multiple columns with code-blocks {.smaller}
:::: {.columns}

::: {.column width="50%"}
```{.python}
#Left column
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```
:::

::: {.column width="50%"}
```{.python}
#Right column
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```
:::

::::

::: {.notes}
Speaker notes go here.
:::







## :construction: Line Highlighting (10 lines)

```{.python filename="matplotlib.py" code-line-numbers="|6-9"}
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

::: {.notes}
Speaker notes go here.
:::











## :construction: Line Highlighting (N lines)
```{.python filename="unit-test-example.py" code-line-numbers="|30-36"}
import datetime
import unittest

import pandas as pd
import pandas_datareader.data as web

def get_stock_data(ticker):
    """pull data from stooq"""
    df = web.DataReader(ticker, 'yahoo')
    return df

class TestGetStockData(unittest.TestCase):
    @classmethod
    def setUpClass(self):
        """We only want to pull this data once for each TestCase since it is an expensive operation"""
        self.df = get_stock_data('^DJI')

    def test_columns_present(self):
        """ensures that the expected columns are all present"""
        self.assertIn("Open", self.df.columns)
        self.assertIn("High", self.df.columns)
        self.assertIn("Low", self.df.columns)
        self.assertIn("Close", self.df.columns)
        self.assertIn("Volume", self.df.columns)

    def test_non_empty(self):
        """ensures that there is more than one row of data"""
        self.assertNotEqual(len(self.df.index), 0)

    def test_high_low(self):
        """ensure high and low are the highest and lowest in the same row"""
        ohlc = self.df[["Open","High","Low","Close"]]
        highest = ohlc.max(axis=1)
        lowest = ohlc.min(axis=1)
        self.assertTrue(ohlc.le(highest, axis=0).all(axis=None))
        self.assertTrue(ohlc.ge(lowest, axis=0).all(axis=None))

    def test_most_recent_within_week(self):
        """most recent data was collected within the last week"""
        most_recent_date = pd.to_datetime(self.df.index[-1])
        self.assertLessEqual((datetime.datetime.today() - most_recent_date).days, 7)

unittest.main()
```

::: {.notes}
Reference for the code!

https://machinelearningmastery.com/a-gentle-introduction-to-unit-testing-in-python/
:::







## :video_camera: Embedding Yotube Video
{{< video https://www.youtube.com/embed/hbf7Ai3jnxY
    start="1269"
    width="85%" 
    height="85%"
>}}

::: {.notes}
Available aspect ratios include 1x1, 4x3, 16x9 (the default), and 21x9.

Further details to render videos
https://quarto.org/docs/authoring/videos.html
:::








<!--
adding HTML comment syntax to not render the following lines
## Embedding a video.mp4 {background-video="video.mp4" background-video-loop="true" background-video-muted="true" background-opacity=0}
-->
